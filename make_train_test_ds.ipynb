{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70167604",
   "metadata": {},
   "source": [
    "# Build train/test HDF5 datasets\n",
    "Merge **two** source HDF5 files – one with normal‑sinus‑rhythm (NSR) recordings, one with atrial‑fibrillation (AF) recordings – into two new self‑contained files:\n",
    "\n",
    "* `train_ds.h5` – all groups from subjects assigned to the *train* split.\n",
    "* `test_ds.h5`  – all groups from the held‑out subjects.\n",
    "\n",
    "Splitting is done **by subject ID** so that no subject appears in both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e61e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/shayanriyaz/miniconda3/envs/AF_DETECTION/lib/python3.12/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/shayanriyaz/miniconda3/envs/AF_DETECTION/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6402601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, pandas as pd, numpy as np, random, os\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44f5ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ---- configuration ----------------------------------------------------\n",
    "# folder_of_interest = \"length_full\"\n",
    "# NSR_PATH   = f\"downloaded_files/{folder_of_interest}/mimic_non_af_data.h5\"\n",
    "# AF_PATH    = f\"downloaded_files/{folder_of_interest}/mimic_af_data.h5\"\n",
    "\n",
    "\n",
    "\n",
    "# OUT_TRAIN  =  f\"downloaded_files/{folder_of_interest}/train_ds.h5\"\n",
    "# OUT_TEST   =  f\"downloaded_files/{folder_of_interest}/test_ds.h5\"\n",
    "\n",
    "# TRAIN_RATIO = 0.8               # 80 % of subjects for training\n",
    "# RNG_SEED    = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf68161",
   "metadata": {},
   "source": [
    "## Helper · collect all groups from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a39a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def collect_groups(path: str, label: int):\n",
    "#     \"\"\"Return list of dicts with file, group_id, subj_id, label.\"\"\"\n",
    "#     rows=[]\n",
    "#     with h5py.File(path, 'r') as h5:\n",
    "#         for gid in h5:\n",
    "#             g = h5[gid]\n",
    "#             subj = str(g.attrs.get('subj_id', gid.split('-')[0]))\n",
    "#             rows.append(dict(file=path, group_id=gid,\n",
    "#                              subj_id=subj, label=label))\n",
    "#     return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af72748",
   "metadata": {},
   "source": [
    "## Step 1 – collect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5faad3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = collect_groups(NSR_PATH, 0) + collect_groups(AF_PATH, 1)\n",
    "# meta = pd.DataFrame(rows)\n",
    "# print('Total groups:', len(meta))\n",
    "# display(meta.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14ba00",
   "metadata": {},
   "source": [
    "## Step 2 – subject‑level split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2726321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random.seed(RNG_SEED)\n",
    "# subjects = meta.subj_id.unique().tolist()\n",
    "# random.shuffle(subjects)\n",
    "\n",
    "# cut = int(len(subjects) * TRAIN_RATIO)\n",
    "# train_subj = set(subjects[:cut])\n",
    "\n",
    "# meta['split'] = np.where(meta.subj_id.isin(train_subj), 'train', 'test')\n",
    "# print(meta.split.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b0715",
   "metadata": {},
   "source": [
    "## Helper · copy selected groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ed696d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def copy_split(df_split, out_path):\n",
    "#     if os.path.exists(out_path):\n",
    "#         os.remove(out_path)\n",
    "#     with h5py.File(out_path, 'w') as h5_out:\n",
    "#         for _, row in tqdm(df_split.iterrows(), total=len(df_split),\n",
    "#                            desc=f'→ {out_path}'):\n",
    "#             with h5py.File(row.file, 'r') as h5_in:\n",
    "#                 src = h5_in[row.group_id]\n",
    "#                 h5_in.copy(src, h5_out, name=row.group_id, without_attrs=False)\n",
    "#                 h5_out[row.group_id].attrs['label'] = row.label\n",
    "#     print(f'Wrote {len(df_split)} groups to {out_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bac4b5",
   "metadata": {},
   "source": [
    "## Step 3 – write new HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16ea2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# ── your existing split lists ───────────────────────────────────────────────\n",
    "# e.g. you might have done something like:\n",
    "# all_ids = sorted(list(h5py.File('mimic_af_data.h5','r').keys()) +\n",
    "#                  list(h5py.File('mimic_non_af_data.h5','r').keys()))\n",
    "# train_ids, test_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "# train_ids = [...]   # <-- fill in your train record IDs\n",
    "# test_ids  = [...]   # <-- fill in your test record IDs\n",
    "\n",
    "# # ── source files ────────────────────────────────────────────────────────────\n",
    "# src_files = [\n",
    "#     'mimic_af_data.h5',\n",
    "#     'mimic_non_af_data.h5'\n",
    "# ]\n",
    "\n",
    "def build_split_h5(src_paths, out_path, selected_ids):\n",
    "    \"\"\"\n",
    "    Copy only groups in selected_ids from each src H5 into a new H5 at out_path,\n",
    "    preserving all dataset contents and attributes.\n",
    "    \"\"\"\n",
    "    with h5py.File(out_path, 'w') as dst:\n",
    "        for src_path in src_paths:\n",
    "            with h5py.File(src_path, 'r') as src:\n",
    "                for gid, grp in src.items():\n",
    "                    if gid not in selected_ids:\n",
    "                        continue\n",
    "                    # replicate group\n",
    "                    dst_grp = dst.create_group(gid)\n",
    "                    # copy group attrs\n",
    "                    for k, v in grp.attrs.items():\n",
    "                        dst_grp.attrs[k] = v\n",
    "                    # copy every dataset in that group\n",
    "                    for ds_name, ds in grp.items():\n",
    "                        if not isinstance(ds, h5py.Dataset):\n",
    "                            continue\n",
    "                        data = ds[()]  # read full array into memory\n",
    "                        dst_ds = dst_grp.create_dataset(\n",
    "                            ds_name,\n",
    "                            data=data,\n",
    "                            compression='gzip',\n",
    "                            chunks=True\n",
    "                        )\n",
    "                        # copy dataset attributes\n",
    "                        for ak, av in ds.attrs.items():\n",
    "                            dst_ds.attrs[ak] = av\n",
    "\n",
    "# # ── run it ──────────────────────────────────────────────────────────────────\n",
    "# build_split_h5(src_files, 'train_ds.h5', train_ids)\n",
    "# build_split_h5(src_files, 'test_ds.h5',  test_ids)\n",
    "\n",
    "# print(\"✔️  Generated train_ds.h5 & test_ds.h5 with preserved structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "168be55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# copy_split(meta[meta.split=='train'], OUT_TRAIN)\n",
    "# copy_split(meta[meta.split=='test'], OUT_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb981565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    28\n",
      "test      7\n",
      "Name: count, dtype: int64\n",
      "✔️  train_ds.h5 and test_ds.h5 created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── your existing configuration ───────────────────────────────\n",
    "folder_of_interest = \"length_full\"\n",
    "NSR_PATH   = f\"downloaded_files/{folder_of_interest}/mimic_non_af_data.h5\"\n",
    "AF_PATH    = f\"downloaded_files/{folder_of_interest}/mimic_af_data.h5\"\n",
    "\n",
    "OUT_TRAIN  = f\"downloaded_files/{folder_of_interest}/train_ds.h5\"\n",
    "OUT_TEST   = f\"downloaded_files/{folder_of_interest}/test_ds.h5\"\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "RNG_SEED    = 42\n",
    "\n",
    "# ── helper to collect all groups with labels ────────────────────\n",
    "def collect_groups(path: str, label: int):\n",
    "    rows = []\n",
    "    with h5py.File(path, 'r') as h5:\n",
    "        for gid in h5:\n",
    "            subj = str(h5[gid].attrs.get('subj_id', gid.split('-')[0]))\n",
    "            rows.append(dict(file=path, group_id=gid, subj_id=subj, label=label))\n",
    "    return rows\n",
    "\n",
    "# ── build a DataFrame and stratified split by subject ───────────\n",
    "rows = collect_groups(NSR_PATH, 0) + collect_groups(AF_PATH, 1)\n",
    "meta = pd.DataFrame(rows)\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "subjects = meta.subj_id.unique().tolist()\n",
    "random.shuffle(subjects)\n",
    "cut = int(len(subjects) * TRAIN_RATIO)\n",
    "train_subj = set(subjects[:cut])\n",
    "\n",
    "meta['split'] = np.where(meta.subj_id.isin(train_subj), 'train', 'test')\n",
    "print(meta.split.value_counts())\n",
    "\n",
    "# ── re‑use your previously defined build_split_h5 ───────────────\n",
    "def build_split_h5(src_paths, out_path, selected_ids):\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    with h5py.File(out_path, 'w') as dst:\n",
    "        for src_path in src_paths:\n",
    "            with h5py.File(src_path, 'r') as src:\n",
    "                for gid, grp in src.items():\n",
    "                    if gid not in selected_ids:\n",
    "                        continue\n",
    "                    # copy entire group (datasets + attrs)\n",
    "                    src.copy(grp, dst, name=gid, without_attrs=False)\n",
    "                    # then ensure our 'label' attr is set/overwritten:\n",
    "                    dst[gid].attrs['label'] = int(grp.attrs.get('af_status', grp.attrs.get('label', 0)))\n",
    "\n",
    "# ── extract group IDs per split and write out ────────────────────\n",
    "train_ids = meta.loc[meta.split=='train', 'group_id'].tolist()\n",
    "test_ids  = meta.loc[meta.split=='test',  'group_id'].tolist()\n",
    "\n",
    "build_split_h5([NSR_PATH, AF_PATH], OUT_TRAIN, train_ids)\n",
    "build_split_h5([NSR_PATH, AF_PATH], OUT_TEST,  test_ids)\n",
    "\n",
    "print(\"✔️  train_ds.h5 and test_ds.h5 created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf57e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AF_DETECTION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
